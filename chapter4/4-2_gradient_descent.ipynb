{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 梯度下降\n",
    "**使用梯度下降时，需要保证所有特征值的大小比例都差不多（比如使用Scikit-Learn的`StandardScaler`类），否则收敛的时间会长很多。**\n",
    "\n",
    "#### 4.2.1 批量梯度下降\n",
    "\n",
    "在计算梯度下降的每一步时，都是基于完整的训练集X的。这就是为什么该算法会被称为批量梯度下降：每一步都使用整批训练数据（实际上，全梯度下降可能是个更好的名字）。因此，面对非常庞大的训练集时，算法会变得极慢（不过我们即将看到快得多的梯度下降算法）。但是，梯度下降算法随特征数量扩展的表现比较好。如果要训练的线性模型拥有几十万个特征，使用梯度下降比标准方程或者SVD要快得多。\n",
    "\n",
    "\n",
    "成本函数的梯度向量\n",
    "\n",
    "$$\n",
    "\\Delta_{\\theta} \\text{MSE}(\\theta) = \\frac{2}{m}X^T(X \\theta - y)       \\tag{4-6}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.05880697],\n",
       "        [0.13833397],\n",
       "        [1.11839701]]),\n",
       " array([[7.46563004],\n",
       "        [4.79332331],\n",
       "        [7.61861999]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "X[:3], y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.05880697],\n",
       "       [1.        , 0.13833397],\n",
       "       [1.        , 1.11839701]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "X_b[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.45409996],\n",
       "       [3.03039606]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 #learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2, 1)\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 随机梯度下降\n",
    "批量梯度下降的主要问题是它要用整个训练集来计算每一步的梯度，所以训练集很大是，算法会特别的慢。与之相反的极端是随机梯度下降，每一步在训练集中随机选择一个实例，并且仅基于该单个实例来计算梯度，可以被用来训练海量的数据集（SGD可以作为核外算法实现）。**由于算法的性质，它比批量梯度下降要不规则的多**\n",
    "\n",
    "![梯度下降的陷阱](./images/gd_trap.png)\n",
    "\n",
    "如上图所示，当loss函数非常不规则时，随机梯度下降其实可是帮助算法跳出局部最小值，所以相比批量梯度下降，它对找到全局最小值更有优势。\n",
    "\n",
    "随机性的好处在于可以逃离局部最优解，但缺点是永远定位不出最小值。要解决这个问题，有一个方法是逐步降低学习率，开始的步长比较长（这有助于快速进展和逃离局部最小值），然后越来越小，让算法尽量靠近全局最小值，**这个过程叫做模拟退火**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.45623151],\n",
       "       [3.017257  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0 /(t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1)\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index: random_index+1]\n",
    "        yi = y[random_index: random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + 1)\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用梯度下降时，训练实例必须独立且均匀分布，以确保平均而言将参数拉向全局最优解。确保这一点的一种简单方法是在训练过程中的对实例进行随机混洗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.41464156]), array([3.02093769]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 小批量梯度下降\n",
    "小批量梯度下降在称为小型批量的随机实例集上计算梯度。小批量梯度下降优于随机梯度下降的主要优点是，你可以通过矩阵操作的硬件优化来提高性能，特别是在使用GPU时。\n",
    "\n",
    "与随机梯度下降相比，该算法在参数空间上的进展更稳定，尤其是在相当大的小批次中。结果，小批量梯度下降最终将比随机梯度下降走得更接近最小值，但它可能很难摆脱局部最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 线性回归算法的比较\n",
    "\n",
    "算法 | m很大 | 核外支持 | n很大 | 超参数 | 要求缩放 | Scikit-Learn\n",
    ":--------:| :--------:| :--------:| :--------:| :--------:| :--------:| :--------:|\n",
    "标准方程 | 快 | 否 | 慢 | 0 | 否 | N/A\n",
    "SVD | 快 | 否 | 慢 | 0 | 否 | LinearRegression\n",
    "批量GD | 慢 | 否 | 快 | 2 | 是 | SGDRegressor\n",
    "随机GD | 快 | 是 | 快 | >= 2 | 是  | SGDRegressor\n",
    "小批量GD | 快 | 是 | 快 | >= 2 | 是 | SGDRegressor\n",
    "\n",
    "**训练后几乎没有区别：所有这些算法最终都具有非常相似的模型，并且以完全相同的方式进行预测。**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "e284c72d79b42194b3fe2a0767ff9cca6d233ae03063bab113c99e4bc6bd25a8"
    }
   },
   "name": "Python 3.7.4 64-bit ('venv')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
