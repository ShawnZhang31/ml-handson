

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>5.4 SVM工作原理 &mdash; 机器学习实战 1.0 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/katex-math.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/translations.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"></script>
        <script src="../_static/katex_autorenderer.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="5.5 课后练习" href="5-5_exercise.html" />
    <link rel="prev" title="5.3 SVM回归" href="5-3_svm_regression.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> 机器学习实战
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction/README.html">0. 机器学习实战</a></li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/ml_overview.html">1. 机器学习概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../c2_end2end/index.html">2. 端到端的机器学习项目</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter3/index.html">3. 分类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter4/index.html">4. 训练模型</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">5. 支持向量机</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="5-1_linear_svm.html">5.1 线性SVM分类</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-2_nonlinear_svm.html">5.2 非线性SVM分类</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-3_svm_regression.html">5.3 SVM回归</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">5.4 SVM工作原理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#5.4.1-决策函数和预测">5.4.1 决策函数和预测</a></li>
<li class="toctree-l3"><a class="reference internal" href="#5.4.2-训练目标">5.4.2 训练目标</a></li>
<li class="toctree-l3"><a class="reference internal" href="#5.4.3-二次规划">5.4.3 二次规划</a></li>
<li class="toctree-l3"><a class="reference internal" href="#5.4.4-对偶问题">5.4.4 对偶问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="#5.4.5-内核化SVM">5.4.5 内核化SVM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#5.4.6-在线SVM">5.4.6 在线SVM</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="5-5_exercise.html">5.5 课后练习</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-5_8_exercise.html">5.5 课后练习-8</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-5_9_exercise.html">5-5 课后练习9</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-5_10_exercise.html">5.5 课后练习10</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter6/index.html">6. 决策树</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter7/index.html">7. 集成学习与随机森林</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">机器学习实战</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">5. 支持向量机</a> &raquo;</li>
        
      <li>5.4 SVM工作原理</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/chapter5/5-4_svm_working_principle.ipynb" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="5.4-SVM工作原理">
<h1>5.4 SVM工作原理<a class="headerlink" href="#5.4-SVM工作原理" title="永久链接至标题">¶</a></h1>
<div class="section" id="5.4.1-决策函数和预测">
<h2>5.4.1 决策函数和预测<a class="headerlink" href="#5.4.1-决策函数和预测" title="永久链接至标题">¶</a></h2>
<p>线性SVM分类器通过简单地计算决策函数来 <span class="math">\(\mathbf{w^T} \mathbf{x} +b = w_1 x_1 + \cdots + w_n x_n + b\)</span> 预测新实例 <span class="math">\(x\)</span> 的分类。如果结果为正，则预测类别是正类（1），否则预测其为负类（0），见公式5-2。</p>
<div class="math">
\[\hat{y} = \begin{cases}
0 & \text{if } \mathbf{w^T} + b \lt 0 \\
1 & \text{if } \mathbf{w^T} + b \ge 0
\end{cases}\]</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">][:,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">iris</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">==</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
((150, 2), (150,))
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">plot_3D_decision_function</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x1_lim</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">x2_lim</span><span class="o">=</span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">]):</span>
    <span class="n">x1_in_bounds</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">x1_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">x1_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">X_crop</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">x1_in_bounds</span><span class="p">]</span>
    <span class="n">y_crop</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">x1_in_bounds</span><span class="p">]</span>
    <span class="n">x1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x1_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x1_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">x2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x2_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x2_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2s</span><span class="p">)</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">x2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">boundary_x2s</span> <span class="o">=</span> <span class="o">-</span><span class="n">x1s</span><span class="o">*</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="n">b</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">margin_x2s_1</span> <span class="o">=</span> <span class="o">-</span><span class="n">x1s</span><span class="o">*</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">margin_x2s_2</span> <span class="o">=</span> <span class="o">-</span><span class="n">x1s</span><span class="o">*</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="p">(</span><span class="n">b</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span>
                    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">boundary_x2s</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;k-&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$h=0$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">margin_x2s_1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;k--&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$h=\pm 1$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1s</span><span class="p">,</span> <span class="n">margin_x2s_2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;k--&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_crop</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y_crop</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_crop</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y_crop</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;g^&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_crop</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][</span><span class="n">y_crop</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_crop</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">][</span><span class="n">y_crop</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;bs&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">x1_lim</span> <span class="o">+</span> <span class="n">x2_lim</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="s2">&quot;Decision function $h$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Petal length&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Petal width&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$h = \mathbf</span><span class="si">{w}</span><span class="s2">^T \mathbf</span><span class="si">{x}</span><span class="s2"> + b$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>


</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">svc_clf2</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>


<span class="n">svc_clf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
LinearSVC(C=100, loss=&#39;hinge&#39;, random_state=42)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">plot_3D_decision_function</span><span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">svc_clf2</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">b</span><span class="o">=</span><span class="n">svc_clf2</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/chapter5_5-4_svm_working_principle_6_0.svg" src="../_images/chapter5_5-4_svm_working_principle_6_0.svg" /></div>
</div>
</div>
<div class="section" id="5.4.2-训练目标">
<h2>5.4.2 训练目标<a class="headerlink" href="#5.4.2-训练目标" title="永久链接至标题">¶</a></h2>
<p>决策函数的斜率：它等于权重向量的范数，即 <span class="math">\(||\mathbf{w}||\)</span> 。如果我们将斜率除以2，那么决策函数等于±1的点也将变得离决策函数两倍远。也就是说，将斜率除以2，将会使间隔乘以2。也许2D图更容易将其可视化，见图5-13。</p>
<p><strong>权重向量w越小，间隔越大</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">plot_2D_decision_function</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">x1_lim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]):</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x1_lim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x1_lim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">m</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">w</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_lim</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k:&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_lim</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k:&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k--&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="n">m</span><span class="p">,</span> <span class="o">-</span><span class="n">m</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k--&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;k-o&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">x1_lim</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ylabel</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1 x_1$  &quot;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w_1 = </span><span class="si">{}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_2D_decision_function</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plot_2D_decision_function</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/chapter5_5-4_svm_working_principle_8_0.svg" src="../_images/chapter5_5-4_svm_working_principle_8_0.svg" /></div>
</div>
<p>我们要最小化 <span class="math">\(||\mathbf{w}||\)</span> 来得到尽可能大的间隔。但是，如果我们想避免任何间隔违例（硬间隔），那么就要使所有正类训练集的决策函数大于1，负类训练集的决策函数小于-1. 如果我们定义实例为负类（如果 <span class="math">\(y^{(i)}=0\)</span> ）时，<span class="math">\(t^{(i)}=-1\)</span> ；实例为正类（如果<span class="math">\(y^{(i)}=1\)</span>）时，<span class="math">\(t^{(i)}=1\)</span>。那么就可以将这个约束条件表示为：对所有实例来说，<span class="math">\(t^{(i)}(\mathbf{w}^T \cdot \mathbf{x}^{(i)} + b) \ge 1\)</span></p>
<p>因此，我们可以将硬间隔线性SVM分类器的目标看作一个约束优化问题:</p>
<p>$$ <span class="math">\underset{\mathbf{w}, b}{\text{minimize}}</span> <span class="math">\frac{1}{2}</span> <span class="math">\mathbf{w}`^T :nbsphinx-math:</span>mathbf{w}`</p>
<p>$$</p>
<p>$$</p>
<p><span class="math">\text{使得}`t\ :sup:`{(i)}(:nbsphinx-math:</span>mathbf{w}``T <span class="math">\cdot `:nbsphinx-math:</span>mathbf{x}`^{(i)} + b) <span class="math">\ge `1, i= 1, 2, :nbsphinx-math:</span>cdots`, m $$</p>
<p>我们要最小化 <span class="math">\(\frac{1}{2} w^T w\)</span> (这等于 <span class="math">\(\frac{1}{2} {||w||}^2\)</span> ), 而不是最小化<span class="math">\(||w||\)</span>， 的确， <span class="math">\(\frac{1}{2} {||w||}^2\)</span>有一个很好的简单的导数(是<span class="math">\(w\)</span>)，而<span class="math">\(||w||\)</span>在<span class="math">\(w=0\)</span>时不可微。</p>
<p>要达到软间隔的目标，我们需要为每一个实例引入一个松弛变量<span class="math">\(\zeta^{(i)} \ge 0\)</span>, <span class="math">\(\zeta^{(i)}\)</span>衡量的是第<span class="math">\(i\)</span>个实例多大程度上允许间隔违例。那么现在我们有了两个互相冲突的目标：使松弛变量越小越好从而减少间隔违例，同时还要使<span class="math">\(w^T \cdot w/2\)</span>最小化以增大间隔。这正是超参数<span class="math">\(C\)</span>的用武之地：允许我们在两个目标之间权衡.</p>
<p>软间隔线性SVM分类器目标:</p>
<p>$$ <span class="math">\underset{\mathbf{w}, b, \zeta}{\text{minimize}}</span> <span class="math">\frac{1}{2}</span> <span class="math">\mathbf{w}`^T :nbsphinx-math:</span>mathbf{w}` + C <span class="math">\sum</span>_{i=1}^m <span class="math">\zeta`^{(i)} :nbsphinx-math:</span>mathbf{w}`</p>
<p>$$</p>
<p>$$</p>
<p><span class="math">\text{使得}`t\ :sup:`{(i)}(:nbsphinx-math:</span>mathbf{w}``T <span class="math">\cdot `:nbsphinx-math:</span>mathbf{x}`^{(i)} + b) <span class="math">\ge `1 - :nbsphinx-math:</span>zeta`^{(i)} <span class="math">\text{和}</span> <span class="math">\zeta`^{(i)} :nbsphinx-math:</span>ge <cite>0, i= 1, 2, :nbsphinx-math:</cite>cdots`, m $$</p>
</div>
<div class="section" id="5.4.3-二次规划">
<h2>5.4.3 二次规划<a class="headerlink" href="#5.4.3-二次规划" title="永久链接至标题">¶</a></h2>
<p>硬间隔和软间隔问题都属于线性约束的凸二次优化问题。这类问题被称为二次规划（QP）问题</p>
<p>公式5-5：二次规划问题</p>
<div class="math">
\[\underset{\mathbf{p}}{\text{minimize}} \frac{1}{2} \mathbf{p}^T \mathbf{H} \mathbf{p} + \mathbf{f}^T \mathbf{p}, \text{使得} \mathbf{A}\mathbf{p} \le b      \tag{5-5}\]</div>
<ul class="simple">
<li><p><span class="math">\(\mathbf{p}\)</span>是一个<span class="math">\(n_p\)</span>维向量（<span class="math">\(n_p\)</span>是参数数量）</p></li>
<li><p><span class="math">\(\mathbf{H}\)</span>是一个<span class="math">\(n_p \times n_p\)</span>的矩阵</p></li>
<li><p><span class="math">\(\mathbf{f}\)</span>是一个<span class="math">\(n_p\)</span>维的向量</p></li>
<li><p><span class="math">\(\mathbf{A}\)</span>是一个<span class="math">\(n_c \times n_p\)</span>的矩阵（<span class="math">\(n_c\)</span>是约束的数量）</p></li>
<li><p><span class="math">\(\mathbf{b}\)</span>是一个<span class="math">\(n_c\)</span>维的向量</p></li>
</ul>
<p>请注意，表达式<span class="math">\(\mathbf{A} \mathbf{p} \le \mathbf{b}^{(i)}\)</span>定义了<span class="math">\(n_c\)</span>个约束：<span class="math">\(p^T a^{(i)} \le b^{(i)}\)</span>，其中<span class="math">\(i=1, 2, \cdots ,n_c\)</span>，其中<span class="math">\(a^{(i)}\)</span>是包含<span class="math">\(\mathbf{A}\)</span>的第<span class="math">\(i\)</span>行元素的向量，<span class="math">\(b^{(i)}\)</span>是<span class="math">\(\mathbf{b}\)</span>的第<span class="math">\(i\)</span>个元素。</p>
<p>你可以轻松地验证，如果用以下方式设置QP参数，就可以得到硬间隔线性SVM分类器的目标:</p>
<ul class="simple">
<li><p><span class="math">\(n_p = n+1\)</span>, 其中n是特征数量（+1是偏置项）</p></li>
<li><p><span class="math">\(n_c = m\)</span>, 其中m是训练的实例的数量</p></li>
<li><p><span class="math">\(H\)</span>是<span class="math">\(n_p \times n_p\)</span>单位矩阵，除了在左上方的元素为零（忽略偏置项）</p></li>
<li><p><span class="math">\(f=0\)</span>是一个全零的<span class="math">\(n_p\)</span>维向量</p></li>
<li><p><span class="math">\(b=-1\)</span>是一个全是-1的<span class="math">\(n_c\)</span>维向量</p></li>
<li><p><span class="math">\(a^{(i)} = t^{(i)} \cdot \dot{x}^{(i)}\)</span>, 其中<span class="math">\(\dot{x}^{(i)}\)</span>等于<span class="math">\(x^{(i)}\)</span>, 并且具有额外的偏差<span class="math">\(\dot{x}_0 = 1\)</span></p></li>
</ul>
<p>所以，要训练硬间隔线性SVM分类器，有一种办法是直接将上面的参数用在一个现成的二次规划求解器上。得到的向量<span class="math">\(p\)</span>将会包括偏置项<span class="math">\(b=p_0\)</span>，以及特征权重<span class="math">\(w_i=p_i, i=1, 2, \cdots ,m\)</span>。类似地，你也可以用二次规划求解器来解决软间隔问题.</p>
</div>
<div class="section" id="5.4.4-对偶问题">
<h2>5.4.4 对偶问题<a class="headerlink" href="#5.4.4-对偶问题" title="永久链接至标题">¶</a></h2>
<p>针对一个给定的约束优化问题，称之为原始问题，我们常常可以用另一个不同的，但是与之密切相关的问题来表达，这个问题我们称之为对偶问题。通常来说，对偶问题的解只能算是原始问题的解的下限，但是在某些情况下，它也可能跟原始问题的解完全相同。幸运的是，SVM问题刚好就满足这些条件(目标函数是凸函数，不等式约束是连续可微和凸函数)，所以你可以选择是解决原始问题还是对偶问题，二者解相同.</p>
<p>公式5-6给出了线性SVM目标的对偶形式:</p>
<div class="math">
\[\underset{\alpha}{\text{minimize}} \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha^{(i)}\alpha^{(j)} t^{(i)} t^{(j)} {\mathbf{x}^{(i)}}^T \mathbf{x}^{(j)} - \sum_{i=1}^m \alpha^{(i)}     \tag{5-6}\]</div>
<p>使得<span class="math">\(\alpha^{(i)} \ge 0, i=1, 2, \cdots, m\)</span></p>
<p>公式5-7：从对偶问题到原始问题</p>
<p>$$ \begin{aligned}</p>
<p><span class="math">\hat{\mathbf{w}}</span> &amp;= <span class="math">\sum</span>_{i=1}^m <span class="math">\hat{\alpha}`^{(i)} t^{(i)} {:nbsphinx-math:</span>mathbf{x}`}^{(i)} \ <span class="math">\hat{b}</span> &amp;= <span class="math">\frac{1}{n_s}</span> <span class="math">\underset{\hat{\alpha}^{(i)} > 0}{\sum_{i=1}^m}</span> (t^{(i)} - <span class="math">\hat{\mathbf{w}}`^T :nbsphinx-math:</span>mathbf{x}`^{(i)})</p>
<p>\end{aligned} $$</p>
<p>当训练实例的数量小于特征数量时，解决对偶问题比原始问题更快速。更重的是，它能够实现核技巧，而原始问题不可能实现。</p>
</div>
<div class="section" id="5.4.5-内核化SVM">
<h2>5.4.5 内核化SVM<a class="headerlink" href="#5.4.5-内核化SVM" title="永久链接至标题">¶</a></h2>
<p>假设二阶多项式的映射函数<span class="math">\(\phi\)</span>如公式5-8所示:</p>
<div class="math">
\[\phi(\mathbf{x}) = \phi \begin{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \end{pmatrix} = \begin{pmatrix}x_1^2 \\ \sqrt{2} x_1 x_2 \\ x_2^2 \end{pmatrix}\]</div>
<p>注意转换后的向量是三维的而不是二维的。现在我们来看看，如果应用这个二阶多项式映射，两个二维向量a和b会发生什么变化，然后计算转换后两个向量的点积（在机器学习中，向量经常表示为列向量，由此点积可以通过计算<span class="math">\(a^T b\)</span>获得）</p>
<p>公式5-9：二阶多项式映射的核技巧：</p>
<div class="math">
\[\phi({\mathbf{a}})^T \phi({\mathbf{b}}) = {\begin{pmatrix} a_1^2 \\ \sqrt{2} a_1 a_2 \\ a_2^2\end{pmatrix}}^T \begin{pmatrix} b_1^2 \\ \sqrt{2} b_1 b_2 \\ b_2^2 \end{pmatrix} = a_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 + a_2^2 + b_2^2 = (a_1 b_1 + a_2 b_2)^2 = {\begin{pmatrix} {\begin{pmatrix} a_1 \\ a_2\end{pmatrix}}^T \begin{pmatrix} b_1 \\ b_2\end{pmatrix} \end{pmatrix}}^2 = (\mathbf{a}^T \mathbf{b})^2\]</div>
<p>转换后向量的点集等于原始向量的点积的平方:</p>
<div class="math">
\[\phi({\mathbf{a}})^T \phi({\mathbf{b}}) = (\mathbf{a}^T \mathbf{b})^2\]</div>
<p>关键点：如果将转换映射<span class="math">\(\phi\)</span>应用于所有训练实例，那么对于对偶问题（见公式5-6）将包含点积:</p>
<div class="math">
\[\phi(x^{(i)})\phi(x^{(j)})\]</div>
<p>的计算。如果<span class="math">\(\phi\)</span>是公式5-8所定义的二阶多项式转换，那么可以直接用:</p>
<div class="math">
\[({\mathbf{x}^{(i)}}^T \mathbf{x}^{(j)})^2\]</div>
<p>来替换这个转换向量的点积。所以你根本不需要转换训练实例，只需要将公式5-6里的点积转换成点积的平方即可。<strong>这个技巧大大调高了整个过程的计算效率，这就是核技巧的本质</strong>。</p>
<p>函数<span class="math">\(K(a, b) = (a^T \cdot b)^2\)</span>被称为二阶多项式核，在机器学习里，核是能够仅基于原始向量a和b来计算点积<span class="math">\(\phi(a)^T\phi(b)\)</span>的函数，它不需要计算（甚至不需要知道）转换函数。</p>
<p>公式5-10是一些常用的核函数:</p>
<div class="math">
\[\begin{aligned}
\text{线性:   } K(\mathbf{a}, \mathbf{b}) &= \mathbf{a}^T \mathbf{b}        \\
\text{多项式: } K(\mathbf{a}, \mathbf{b}) &= (\gamma \mathbf{a}^T \mathbf{b} + r)^d     \\
\text{高斯RBF: } K(\mathbf{a}, \mathbf{b}) &= \exp(- \gamma {||\mathbf{a} - \mathbf{b}||}^2 )     \\
\text{Sigmoid: } K(\mathbf{a}, \mathbf{b}) &= \tanh( \gamma \mathbf{a}^T \mathbf{b} + r )     \\
\end{aligned}\]</div>
<p>根据Mercer定理，如果函数<span class="math">\(K(a, b)\)</span>符合几个数学条件：也就是Mercer条件（K必须是连续的，并且在其参数上对称，所以<span class="math">\(K(a, b)=K(b, a)\)</span>, 等等），则存在函数<span class="math">\(\phi\)</span>将a和b映射到另一个空间（可能是更高维度的空间），使得<span class="math">\(K(a, b) = \phi(a)^T \phi(b)\)</span>. 所以你可以将K用作核函数，因为你知道<span class="math">\(\phi\)</span>是存在的。</p>
<p>注意，也有一些常用的核函数（如S型核函数）不符合Mercer条件的所有条件，但是他们在实践中通常也表现不存。</p>
<p>还有一个未了解的问题需要说明。公式5-7显示了用线性SVM分类器如何从对偶解走向原始解，但是如果你应用了核技巧，最终得到的是包含<span class="math">\(\phi(x^{(i)})\)</span>的方程。而<span class="math">\(\hat{\mathbf{w}}\)</span>的维度数量必须与<span class="math">\(\phi(x^{(i)})\)</span>相同，后者很有可能是巨大甚至是无穷大的，所以你根本无法计算，可是不知道<span class="math">\(\hat{\mathbf{w}}\)</span>该如何做出预测呢？你可以将公式5-7中的<span class="math">\(\hat{\mathbf{w}}\)</span>的公式插入新实例<span class="math">\(x^{(n)}\)</span>的决策函数中，这样就得到了一个只能包含输入向量之间点积的公式。这时你就可以再次运用核技巧了:</p>
<p>公式5-11: 使用核化SVM做出预测:</p>
<div class="math">
\[\begin{aligned}
h_{\hat{\mathbf{w}}, \hat{b}} (\phi(\mathbf{x}^{(n)})) &= \hat{\mathbf{w}}^T \phi(\mathbf{x}^{(n)}) + \hat{b} \\
&= \begin{pmatrix} \sum_{i=1}^m \hat{\alpha}^{(i)}t^{(i)}\phi(\mathbf{x}^{(i)})\end{pmatrix}^T \phi(\mathbf{x}^{(n)}) + \hat{b}     \\
&= \sum_{i=1}^m \hat{\alpha}^{(i)} t^{(i)} (\phi(\mathbf{x}^{(i)})^T \phi(\mathbf{x}^{(n)})) + \hat{b}      \\
&= \underset{\hat{\alpha}^{(i)} > 0}{\sum_{i=1}^m} \hat{\alpha}^{(i)} t^{(i)} K(\mathbf{x}^{(i)}, \mathbf{x}^{(n)}) + \hat{b}
\end{aligned}\]</div>
<p>注意，因为仅对于支持向量才有<span class="math">\(\alpha^{(i)} \ne 0\)</span>， 所以在预测时，计算新输入向量<span class="math">\(\mathbf{x}(n)\)</span>的点积，使用的仅仅是支持向量而不是全部训练实例。当然，你还需要使用同样的技巧计算偏置项<span class="math">\(\hat{b}\)</span>.</p>
<p>公式5-12：使用核技巧来计算偏置项:</p>
<p>$$ \begin{aligned} <span class="math">\hat{b}</span> &amp;= <span class="math">\frac{1}{n_s}</span> <span class="math">\underset{\hat{\alpha}^{(i)}>0}{\sum_{i=1}^m}</span> (t^{(i)} - <span class="math">\hat{w}`^T :nbsphinx-math:</span>phi`(<span class="math">\mathbf{x}`^{(i)})) \\ &= :nbsphinx-math:</span>frac{1}{n_s}` <span class="math">\underset{\hat{\alpha}^{(i)}>0}{\sum_{i=1}^m}</span> <span class="math">\Bigg`( t^{(i)} - :nbsphinx-math:</span>Bigg`( <span class="math">\sum</span>_{j=1}^m <span class="math">\hat{\alpha}`^{(j)} t^{(j)}
:nbsphinx-math:</span>phi`(<span class="math">\mathbf{x}`^{(j)}) :nbsphinx-math:</span>Bigg`)^T <span class="math">\phi`(:nbsphinx-math:</span>mathbf{x}`^{(i)}) <span class="math">\Bigg</span>) \ &amp;= <span class="math">\frac{1}{n_s}</span> <span class="math">\underset{\hat{\alpha}^{(i)}>0}{\sum_{i=1}^m}</span> <span class="math">\Bigg`( t^{(i)} - :nbsphinx-math:</span>underset{hat{alpha}^{(j)}&gt;0}{sum_{j=1}^m}` <span class="math">\hat{\alpha}`^{(j)} t^{(j)} K(:nbsphinx-math:</span>mathbf{x}`^{(i)}, <span class="math">\mathbf{x}`^{(j)}) :nbsphinx-math:</span>Bigg`)</p>
<p>\end{aligned} $$</p>
<p>如果你现在觉得开始头痛，完全正常：这正是核技巧的副作用</p>
</div>
<div class="section" id="5.4.6-在线SVM">
<h2>5.4.6 在线SVM<a class="headerlink" href="#5.4.6-在线SVM" title="永久链接至标题">¶</a></h2>
<p>对于线性SVM分类器，一种实现在线SVM分类器的方法是使用梯度下降（例如，使用SGDClassifier）来最小化源自原始问题的公式5-13的成本函数。不幸的是，梯度下降比基于QP的方法收敛慢得多。</p>
<p>公式5-13：线性SVM分类器成本函数:</p>
<div class="math">
\[J(\mathbf{w}, b) = \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \sum_{i=1}^m \max(0, 1-t^{(i)}(\mathbf{w}^T \mathbf{x}^{(i)}+b))     \tag{5-13}\]</div>
<p>成本函数是的第一项会推动模型得到一个较小的权重向量<span class="math">\(\mathbf{w}\)</span>， 从而使间隔更大。第二项则计算全部的间隔违例。如果没有一个示例位于街道之上，并且都在街道正确的一边，那么各个实例的间隔违例为0；否则，该实例的违例大小与其到街道正确一边的距离成正比。所以将这个项最小化，能够保证模型使间隔违例尽可能小，也尽可能少。</p>
<p>Hinge损失函数</p>
<p>函数<span class="math">\(\max(0, 1-t)\)</span>被称为hinge损失函数。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\max(0, 1-t)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/chapter5_5-4_svm_working_principle_30_0.svg" src="../_images/chapter5_5-4_svm_working_principle_30_0.svg" /></div>
</div>
<p>在线SVM也可是实现核技巧，可参考“Incremental and Decremental SVM Learning”, 以及“Fast Kernel Classifier with Online and Active Learning”。但是这些核SVM都是在Matlab和C++上实现的，对于大规模非线性问题，你可能需要使用神经网络。</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="5-5_exercise.html" class="btn btn-neutral float-right" title="5.5 课后练习" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="5-3_svm_regression.html" class="btn btn-neutral float-left" title="5.3 SVM回归" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2020, 守着瓜的猹.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>