{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 8.7 练习Q&A"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. 减少数据集维度的主要动机是什么？主要缺点是什么？\n",
    "\n",
    "- 主要动机：\n",
    "    - 为了加速后续的训练算法（在某些情况下，也可能为了消除噪声和冗余特征，使训练算法性能更好）      \n",
    "    - 为了将数据可视化，并从中获得洞见，了解最重要的特征        \n",
    "    - 为了节省空间（压缩）\n",
    "\n",
    "- 主要弊端：\n",
    "    - 丢失部分信息，可能使后续训练算法的性能降低\n",
    "    - 可能是计算密集型的\n",
    "    - 为机器学习流水线增添了些许复杂度\n",
    "    - 转换后的特征难以解释"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2. 维度的诅咒是什么？\n",
    "\n",
    "维度的诅咒是指许多在低维空间中不存在的问题，在高维空间中发生。在机器学习领域，一个常见的现象是随机抽样的高维向量通常非常稀疏，提升了过拟合的风险，同时也使得在没有充足训练数据的情况下，要识别数据中的模式非常困难\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3. 一旦降低了数据集的维度，是否可以逆操作？如果可以，怎么做？如果不能，为什么？\n",
    "\n",
    "一旦使用我们讨论的任意算法减少了数据集的维度，就几乎不可能再将操作完美地逆转，因为在降维过程中必然丢失了一部分信息。此外，虽然有一些算法（例如PCA）拥有简单的逆转换过程，可以重建出与原始数据集相似的数据集，但是也有一些算法不能实现逆转（例如T-SNE）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4. 可以使用PCA来减少高度非线性的数据集的维度吗？\n",
    "\n",
    "对大多数数据集来说，PCA可以用来进行显著降维，即便是高度非线性的数据集，因为它至少可以消除无用的维度。但是如果不存在无用的维度（例如瑞士卷），那么使用PCA降维将会损失太多信息。你希望的是将瑞士卷展开，而不是将其压扁。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "5. 假设你在1000维的数据集上执行PCA，将可解释方差比设置为95%。结果数据集将具有多少个维度？\n",
    "\n",
    "这是一个不好回答的问题，它取决于数据集。我们来看看两个极端的示例。首先，假设数据集是由几乎完全对齐的点组成的，在这种情况下，PCA可以将数据集降至一维，同时保留95%的方差。现在，试想数据集由完全随机的点组成，分散在1000个维度上，在这种情况下，需要在950个维度上保留95%的方差。所以，这个问题的答案是：取决于数据集，它可能是1到950之间的任何数字。将解释方差绘制成关于维度数量的函数，可以对数据集的内在维度获得一个粗略的概念。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "6. 在什么情况下，你将使用常规PCA、增量PCA、随机PCA或内核PCA？\n",
    "\n",
    "常规PCA是默认选择，但是它仅适用于内存足够处理训练集的时候。增量PCA对于内存无法支持的大型数据集非常有用，但是它比常规PCA要慢一些，所以如果内存能够支持，还是应该使用常规PCA。当你需要随时应用PCA来处理每次新增的实例时，增量PCA对于在线任务同样有用。当你想大大降低维度数量，并且内存能够支持数据集时，使用随机PCA非常有效，它比常规PCA快得多。最后，对于非线性数据集，使用核化PCA非常有效。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "7. 如何评估数据集中的降维算法的性能？\n",
    "\n",
    "直观来说，如果降维算法能够消除许多维度并且不会丢失太多信息，那么这就算一个好的降维算法。**进行衡量的方法之一是应用逆转换然后测量重建误差。然而并不是所有的降维算法都提供了逆转换。还有另一种选择，如果你将降维当作一个预处理过程，用在其他机器学习算法（比如随机森林分类器）之前，那么可以通过简单测量第二个算法的性能来进行评估。如果降维过程没有损失太多信息，那么第二个算法的性能应该跟使用原始数据集一样好。**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "8. 链接两个不同的降维算法是否有意义？\n",
    "\n",
    "**链接两个不同的降维算法绝对是有意义的。**常见的示例是使用PCA快速去除大量无用的维度，然后应用另一种更慢的降维算法，如LLE。这种两步走的策略产生的结果可能与仅使用LLE相同，但是时间要短的多。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}