{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 6.10 练习"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. 如果训练集有100万个实例，训练决策树（无约束）大致的深度是多少？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "一个包含 $m$ 个叶子节点的均衡二叉树的深度等于 $\\log_2(m)$ 取整。通常来说，二元决策树（只做二元决策的树，就像Scikit-Learn中的所有树一样）训练到最后大体都是平衡的，如果不加限制，最后平均每个叶子点一个实例。因此，如果训练集包含100万个实例，那么决策树的深度为 $\\log_2(10^6) \\approx 20$ 层（**实际上会更多一些，因为决策树通常不可能完美平衡**）。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2.通常来说，子节点的基尼不纯度是高于还是低于其父节点？是通常更高/更低？还是永远更高/更低？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.如果决策树过拟合训练集，减少max_depth是否为一个好主意？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4.如果决策树对训练集欠拟合，尝试缩放输入特征是否为一个好主意？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "5.如果在包含100万个实例的训练集上训练决策树需要一个小时，那么在包含1000万个实例的训练集上训练决策树，大概需要多长时间？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "6.如果训练集包含10万个实例，设置presort=True可以加快训练吗？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "7.为卫星数据集训练并微调一个决策树。\n",
    "a.使用make_moons（n_samples=10000，noise=0.4）生成一个卫星数据集。\n",
    "b.使用train_test_split（）拆分训练集和测试集。\n",
    "c.使用交叉验证的网格搜索（在GridSearchCV的帮助下）为DecisionTreeClassifier找到适合的超参数。提示：尝试max_leaf_nodes的多种值。\n",
    "d.使用超参数对整个训练集进行训练，并测量模型在测试集上的性能。你应该得到约85%～87%的准确率。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "8.按照以下步骤种植森林。\n",
    "a.继续之前的练习，生产1000个训练集子集，每个子集包含随机挑选的100个实例。提示：使用Scikit-Learn的ShuffleSplit来实现。\n",
    "b.使用前面得到的最佳超参数值，在每个子集上训练一个决策树。在测试集上评估这1000个决策树。因为训练集更小，所以这些决策树的表现可能比第一个决策树要差一些，只能达到约80%的准确率。\n",
    "c.见证奇迹的时刻到了。对于每个测试集实例，生成1000个决策树的预测，然后仅保留次数最频繁的预测（可以使用SciPy的mode（）函数）。这样你在测试集上可获得大多数投票的预测结果。\n",
    "d.评估测试集上的这些预测，你得到的准确率应该比第一个模型更高（高出0.5%～1.5%）。恭喜，你已经训练出了一个随机森林分类器！"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}